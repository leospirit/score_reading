"""
Wav2Vec2-GOP è¯„åˆ†å¼•æ“ - ä½¿ç”¨ Transformers å®ç°ç°ä»£å¼ºåˆ¶å¯¹é½ä¸è¯„åˆ†

è¯¥å¼•æ“ä»£æ›¿ Kaldiï¼Œåˆ©ç”¨ Wav2Vec2 æ¨¡å‹æå–éŸ³ç´ åéªŒæ¦‚ç‡ï¼Œå¹¶ç»“åˆ CTC å¯¹é½ç®—æ³•å®ç°ç²¾å‡†åˆ‡ç‰‡ã€‚
"""
import logging
import os
import torch
import torchaudio
import numpy as np
import librosa
from pathlib import Path
from typing import Any, Optional, Dict, List
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

try:
    from num2words import num2words
except ImportError:
    num2words = None

# DeepFilterNet for noise reduction
try:
    from df.enhance import enhance, init_df
    HAS_DF = True
except ImportError:
    HAS_DF = False

from src.config import config
from src.models import (
    Alignment,
    PhonemeAlignment,
    PhonemeTag,
    WordAlignment,
    WordTag,
)

logger = logging.getLogger(__name__)

class Wav2Vec2Engine:
    """
    Wav2Vec2 GOP è¯„åˆ†å¼•æ“
    """
    
    def __init__(self) -> None:
        model_id = config.get("engines.wav2vec2.model", "facebook/wav2vec2-base-960h")
        self.device = config.get("engines.wav2vec2.device", "cpu")
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            
        logger.info(f"æ­£åœ¨åŠ è½½ Wav2Vec2 æ¨¡å‹: {model_id} (Device: {self.device})")
        
        try:
            self.processor = Wav2Vec2Processor.from_pretrained(model_id)
            self.model = Wav2Vec2ForCTC.from_pretrained(model_id).to(self.device)
            self.model.eval()
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"Wav2Vec2 åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¨¡å‹åç§°: {model_id}")
            
        # åˆå§‹åŒ–é™å™ªæ¨¡å‹
        if HAS_DF:
            logger.info("Initializing DeepFilterNet for noise reduction...")
            self.df_state = init_df()
        else:
            self.df_state = None
            logger.warning("DeepFilterNet not installed, skipping advanced noise reduction.")

    def run(
        self,
        wav_path: Path,
        script_text: str,
        work_dir: Optional[Path] = None,
    ) -> tuple[Alignment, dict[str, Any]]:
        """
        è¿è¡Œå¯¹é½ä¸è¯„åˆ†é€»è¾‘
        """
        waveform, sample_rate = torchaudio.load(str(wav_path))
        
        # å¼ºåˆ¶è½¬æ¢ä¸ºå•å£°é“ (Avoid stereo processing errors)
        if waveform.shape[0] > 1:
            logger.info(f"Converting stereo audio ({waveform.shape[0]} channels) to mono.")
            waveform = torch.mean(waveform, dim=0, keepdim=True)
            
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        waveform = waveform.squeeze().to(self.device)
        
        # 1.5 é™å™ªå¢å¼º (DeepFilterNet)
        if HAS_DF and self.df_state:
            try:
                # DeepFilterNet æœŸæœ›çš„æ•°æ®æ˜¯ 2-dim (C, T) ä¸”é‡‡æ ·ç‡é€šå¸¸æ˜¯ 48k
                # ä½†å®ƒä¹Ÿæ”¯æŒä¸åŒé‡‡æ ·ç‡ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨é‡é‡‡æ ·
                # æˆ‘ä»¬å…ˆå°† 16k ä¿¡å·è½¬ä¸º 48k å¢å¼ºï¼Œå†è½¬å› 16k
                logger.info("Applying DeepFilterNet enhancement...")
                
                # waveform ç›®å‰æ˜¯ (T,) 16k
                # è½¬ä¸º (1, T)
                wf_in = waveform.unsqueeze(0).cpu()
                
                # ä½¿ç”¨ librosa é‡é‡‡æ ·åˆ° 48k (DF æœ€ä½³é‡‡æ ·ç‡)
                wf_48k = librosa.resample(wf_in.numpy()[0], orig_sr=16000, target_sr=48000)
                wf_48k_tensor = torch.from_numpy(wf_48k).unsqueeze(0)
                
                # å¢å¼º
                enhanced_48k = enhance(self.model_df if hasattr(self, 'model_df') else self.df_state, self.df_state, wf_48k_tensor)
                
                # è½¬å› 16k
                enhanced_16k = librosa.resample(enhanced_48k.numpy()[0], orig_sr=48000, target_sr=16000)
                waveform = torch.from_numpy(enhanced_16k).to(self.device)
                
                logger.info("Noise reduction complete.")
            except Exception as e:
                logger.warning(f"Noise reduction failed, proceeding with original audio: {e}")
        
        # 2. è·å– Logits
        with torch.no_grad():
            inputs = self.processor(waveform, sampling_rate=16000, return_tensors="pt", padding=True)
            input_values = inputs.input_values.to(self.device)
            logits = self.model(input_values).logits
            
        # è½¬æ¢æ¦‚ç‡
        log_probs = torch.nn.functional.log_softmax(logits, dim=-1).squeeze(0).cpu().numpy()
        
        # 3. CTC å¼ºåˆ¶å¯¹é½ (ç®€åŒ–çš„å¯¹é½å®ç°)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸º IDã€‚Wav2Vec2 é»˜è®¤åŸºäº Characterã€‚
        # å¯¹äºå‘éŸ³è¯„ä¼°ï¼Œç†æƒ³æƒ…å†µæ˜¯åŸºäº Phoneme æ¨¡å‹ï¼Œè¿™é‡Œå…ˆå®ç° Character å¯¹é½ä½œä¸ºåŸºç¡€ã€‚
        target_text = script_text.upper()
        alignment = self._forced_align(log_probs, target_text)
        
        # æ±‡æ€»ä¸ºå•è¯çº§è¯„åˆ†
        alignment_obj, engine_raw = self._process_alignment_results(alignment, log_probs, script_text)
        
        # è®¡ç®—é«˜çº§ç»´åº¦
        duration = alignment_obj.words[-1].end if alignment_obj.words else 0
        fluency_score, fluency_stats = self._calculate_fluency(alignment_obj.words, duration)
        intonation_score, intonation_stats = self._calculate_intonation(wav_path)
        
        engine_raw["pronunciation_score"] = engine_raw["overall_score"]
        engine_raw["fluency_score"] = fluency_score
        engine_raw["intonation_score"] = intonation_score
        engine_raw["completeness_score"] = 100
        
        engine_raw.update(fluency_stats)
        engine_raw.update(intonation_stats)
        
        return alignment_obj, engine_raw

    def _forced_align(self, log_probs: np.ndarray, target_text: str) -> List[Dict]:
        """
        æç®€ CTC å¯¹é½é€»è¾‘ (Viterbi è·¯å¾„æœç´¢)
        """
        # 1. é¢„å¤„ç†æ–‡æœ¬ï¼šè½¬å¤§å†™ï¼Œå¤„ç†æ•°å­—ï¼Œç§»é™¤æ ‡ç‚¹
        # Wav2Vec2 è¯è¡¨é€šå¸¸æ²¡æœ‰æ ‡ç‚¹ (é™¤äº† ' å’Œ -) å’Œ æ•°å­—
        import re
        
        def _normalize_text(text: str) -> str:
            # Convert numbers to words (e.g., "25" -> "twenty five")
            if num2words:
                tokens = []
                for word in text.split():
                    if word.isdigit():
                        try:
                            tokens.append(num2words(int(word), lang='en'))
                        except:
                            tokens.append(word)
                    else:
                        tokens.append(word)
                text = " ".join(tokens)
            
            # Remove punctuation except apostrophes
            text = re.sub(r"[^A-Za-z' ]", " ", text.upper())
            # Collapse spaces
            return re.sub(r"\s+", " ", text).strip()

        normalized_script = _normalize_text(target_text)
        raw_words = re.findall(r"[\w']+", normalized_script)
        processed_text = "|".join(raw_words)
        
        tokens = self.processor.tokenizer.tokenize(processed_text)
        token_ids = self.processor.tokenizer.convert_tokens_to_ids(tokens)
        
        T = log_probs.shape[0]
        N = len(token_ids)
        
        # å¯¹é½çŸ©é˜µ (DP)
        dp = np.full((T, N), -np.inf)
        backtrack = np.zeros((T, N), dtype=int)
        
        # åˆå§‹åŒ–
        dp[0][0] = log_probs[0][token_ids[0]]
        
        for t in range(1, T):
            for n in range(N):
                # çŠ¶æ€è½¬ç§»ï¼šä¿ç•™å½“å‰çŠ¶æ€ or ä»ä¸Šä¸€ä¸ªçŠ¶æ€è½¬ç§» or è·³è¿‡å½“å‰çŠ¶æ€ (Skip-Token)
                p_stay = dp[t-1][n]
                p_move = dp[t-1][n-1] if n > 0 else -np.inf
                # Skip-Token: å…è®¸è·³è¿‡ä¸€ä¸ª Token (ä¾‹å¦‚ | æˆ–æçŸ­è¯)
                p_skip = dp[t-1][n-2] if n > 1 else -np.inf
                
                # æƒé‡è°ƒæ•´ï¼šç§»åŠ¨ > åœé¡¿ >> è·³è¿‡
                p_move += 0.2    # å¢å¼ºç§»åŠ¨æ„æ„¿ï¼Œé˜²æ­¢åœ¨ä¸€ä¸ª Token åœç•™è¿‡ä¹…
                p_skip -= 15.0   # æå¤§å¢åŠ è·³è¿‡æƒ©ç½š (ä» 5.0 -> 15.0)ï¼Œå¼ºåˆ¶ç®—æ³•ä¼˜å…ˆç•™åœ¨åŸå‰§æœ¬è·¯å¾„ä¸Š
                
                if p_move >= p_stay and p_move >= p_skip:
                    dp[t][n] = p_move + log_probs[t][token_ids[n]]
                    backtrack[t][n] = n - 1
                elif p_stay >= p_skip:
                    dp[t][n] = p_stay + log_probs[t][token_ids[n]]
                    backtrack[t][n] = n
                else:
                    dp[t][n] = p_skip + log_probs[t][token_ids[n]]
                    backtrack[t][n] = n - 2
                    
        # å›æº¯è·¯å¾„
        path = []
        # CRITICAL: æ‰¾åˆ°æœ€åä¸€å¸§å¾—åˆ†æœ€é«˜ä¸”æœ€é åçš„ Token (å¤„ç†æœªå®Œå…¨å¯¹é½çš„æƒ…å†µ)
        best_n = np.argmax(dp[T-1, :])
        curr_n = int(best_n)
        
        for t in range(T - 1, -1, -1):
            path.append((t, curr_n))
            curr_n = int(backtrack[t][curr_n])
            if curr_n < 0: curr_n = 0
        path.reverse()
        
        # è½¬æ¢ä¸ºæ—¶é—´è½´æ®µ
        segments = []
        for n in range(N):
            frames = [t for t, token_idx in path if token_idx == n]
            
            if frames:
                # Wav2Vec2 å¸§é•¿é€šå¸¸çº¦ä¸º 20ms
                start_s = min(frames) * 0.02
                end_s = (max(frames) + 1) * 0.02
                
                # è®¡ç®— GOP (æ ¡å‡†ç‰ˆ)
                frame_probs = [log_probs[t][token_ids[n]] for t in frames]
                frame_max_probs = [np.max(log_probs[t]) for t in frames]
                gop = np.mean(np.array(frame_probs) - np.array(frame_max_probs))
            else:
                # CRITICAL FIX: å¦‚æœè¯¥ Token æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å¸§ï¼ˆå¸¸è§äºå‘éŸ³æå·®æˆ–çŸ­ä¿ƒè¯ï¼‰
                # èµ‹äºˆä¸€ä¸ªæå°çš„æŒç»­æ—¶é—´ï¼Œä½ç½®å‚è€ƒä¸Šä¸€ä¸ªæœ‰æ•ˆå¸§ï¼Œé˜²æ­¢ç´¢å¼•é”™ä½å¯¼è‡´åé¢çš„è¯å…¨éƒ¨æ ‡è®°ä¸ºâ€œæ¼è¯»â€
                prev_end = segments[-1]["end"] if segments else 0.0
                start_s = prev_end
                end_s = prev_end + 0.01 # 10ms å ä½
                gop = -10.0 # æä½åˆ†è¡¨ç¤ºæœªæ£€æµ‹åˆ°æœ‰æ•ˆå‘éŸ³
            
            segments.append({
                "token": tokens[n],
                "start": start_s,
                "end": end_s,
                "gop": gop
            })
            
        # å¦‚æœå¯¹é½å¤±è´¥æˆ–ä¸¥é‡ç¼ºå¤±ï¼ˆæ¯”å¦‚åªè¯†åˆ«äº†ä¸åˆ° 85% çš„è¯ï¼‰ï¼Œåˆ™å¯åŠ¨çº¿æ€§å…œåº•
        # æé«˜é˜ˆå€¼ä»¥ç¡®ä¿ç”¨æˆ·ä½“éªŒï¼Œå®æ„¿ç»™ä¸ªå¤§æ¦‚åˆ†ä¹Ÿä¸è¦æ¼è¯»
        if not segments or (len(segments) < len(tokens) * 0.85):
             logger.warning(f"CTC Alignment poor (Found {len(segments)} segments vs {len(tokens)} tokens). Triggering Linear Fallback.")
             segments = self._linear_alignment_fallback(tokens, log_probs.shape[0] * 0.02)

        return segments

    def _linear_alignment_fallback(self, tokens: List[str], duration: float) -> List[Dict]:
        """
        çº¿æ€§å…œåº•å¯¹é½ï¼šå°†æ‰€æœ‰ Token (åŒ…æ‹¬åˆ†éš”ç¬¦ |) å‡åŒ€åˆ†å¸ƒåœ¨éŸ³é¢‘æ—¶é—´è½´ä¸Šã€‚
        å¿…é¡»ä¿ç•™ |ï¼Œå¦åˆ™ä¸‹æ¸¸æ— æ³•ç»„è¯ã€‚
        """
        segments = []
        if not tokens:
             return []
        
        # å‡è®¾é¦–å°¾å„ç•™ 0.2s é™éŸ³
        margin = min(0.2, duration * 0.05)
        start_t = margin
        end_t = max(margin + 0.1, duration - margin)
        
        # è®¡ç®—æ¯ä¸ª Token çš„ avg duration
        # æ³¨æ„ï¼štokens åŒ…å« |
        n_tokens = len(tokens)
        step = (end_t - start_t) / n_tokens
        
        for i, token in enumerate(tokens):
            t_start = start_t + i * step
            t_end = t_start + step
            
            # å¦‚æœæ˜¯åˆ†éš”ç¬¦ |ï¼Œé€šå¸¸æ—¶é—´å¾ˆçŸ­æˆ–é™„ç€åœ¨å‰ä¸€ä¸ªè¯ï¼Ÿ
            # ç®€å•èµ·è§ï¼Œçº¿æ€§åˆ†é…å³å¯ã€‚ä¸‹æ¸¸é€»è¾‘ handle checking |
            
            # GOP åˆ†æ•°ï¼šç»™ä¸€ä¸ªæ¯”è¾ƒå¥½çš„åˆ†æ•°ï¼Œä¾‹å¦‚ 80 åˆ† -> GOP approx -1.5 ~ -2.0 ?
            # logic: 100 + gop * 9.5 >= 80 => gop * 9.5 >= -20 => gop >= -2.1
            gop_score = -1.5 
            
            segments.append({
                "token": token,
                "start": t_start,
                "end": t_end,
                "gop": gop_score
            })
            
        return segments


    def _calculate_fluency(self, words: List[WordAlignment], duration_sec: float) -> tuple[float, dict]:
        """
        è®¡ç®—æµåˆ©åº¦ (WPM + Pauses)
        """
        if not words or duration_sec <= 0:
            return 0.0, {}
            
        # 1. WPM Calculation
        num_words = len(words)
        wpm = (num_words / duration_sec) * 60
        
        # 2. Pause Detection
        pauses = []
        total_pause_duration = 0.0
        
        for i in range(len(words) - 1):
            gap = words[i+1].start - words[i].end
            if gap > 0.3: # 300ms threshold for pause
                pauses.append(gap)
                total_pause_duration += gap
                
        # 3. Scoring
        # Target WPM: 80-130 for reading. (Lowered from 110 baseline)
        # Score = 100 - penalty
        wpm_score = min(100, (wpm / 90) * 100) if wpm < 90 else 100
        if wpm > 170: # Too fast penalty
             wpm_score -= (wpm - 170) * 0.3
             
        # Scale penalties by duration to be fairer for long audio
        # Reduced penalty coefficients (2 -> 1, 5 -> 2)
        pause_penalty = (len(pauses) * 1.0) + (total_pause_duration * 2.0)
        
        fluency_score = float(np.clip(wpm_score - pause_penalty, 0, 100))
        
        return fluency_score, {
            "wpm": wpm,
            "pause_count": len(pauses),
            "total_pause_duration": total_pause_duration
        }

    def _calculate_intonation(self, wav_path: Path) -> tuple[float, dict]:
        """
        è®¡ç®—è¯­è°ƒåˆ†æ•° (åŸºäº F0 å’Œ èƒ½é‡æ ‡å‡†å·®)
        """
        try:
            y, sr = librosa.load(str(wav_path), sr=16000)
            
            # 1. Energy Variation (RMSE)
            rmse = librosa.feature.rms(y=y)[0]
            energy_std = np.std(rmse)
            
            # 2. Pitch Variation (F0) - using pyin (can be slow, use checks)
            # optimizations: limit duration or frame length if needed
            if len(y) / sr > 150: # Increased from 30 to 150s (2.5 mins) per user request
                f0_std = 0
                f0 = np.array([])
            else:
                 # Reduce n_fft for long audio to save memory/time
                 f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), frame_length=1024)
                 f0 = f0[~np.isnan(f0)]
                 f0_std = np.std(f0) if len(f0) > 0 else 0
                 
            # Scoring logic
            # Energy std typically 0.01 - 0.05 for good speech
            energy_score = min(100, (energy_std / 0.02) * 80)
            
            # F0 std typically 20-50Hz for expressive speech
            pitch_score = min(100, (f0_std / 30) * 90) if f0_std > 0 else 0
            
            # Combined score (Fallback to energy if pitch failed)
            final_score = (original_score := (0.4 * energy_score + 0.6 * pitch_score) if pitch_score > 0 else energy_score)
             
            # Construct Pitch Contour (Downsample for JSON/UI size)
            pitch_contour = []
            if len(f0) > 0:
                times = librosa.times_like(f0, sr=sr, hop_length=512)
                # Take every 10th point to reduce size
                for i in range(0, len(f0), 10):
                    if i < len(times):
                        pitch_contour.append({"t": float(times[i]), "f": float(f0[i])})

            return float(np.clip(final_score, 0, 100)), {
                "energy_std": float(energy_std),
                "f0_std": float(f0_std),
                "pitch_contour": pitch_contour
            }
            
        except Exception as e:
            logger.warning(f"Intonation calc failed: {e}")
            return 70.0, {"pitch_contour": []} # Fallback

    def _process_alignment_results(self, segments: List[Dict], log_probs: np.ndarray, script_text: str) -> tuple[Alignment, dict[str, Any]]:
        """
        è½¬æ¢å¯¹é½æ®µåˆ° Alignment æ¨¡å‹
        """
        result = Alignment()
        
        # æŒ‰ | ç»„åˆå•è¯
        words_data = []
        current_word_tokens = []
        
        for seg in segments:
            if seg["token"] == "|":
                if current_word_tokens:
                    words_data.append(current_word_tokens)
                    current_word_tokens = []
            else:
                current_word_tokens.append(seg)
        if current_word_tokens:
            words_data.append(current_word_tokens)
            
        import re
        raw_words = re.findall(r"[\w']+", script_text.upper())
        
        weak_words_list = []
        
        # ç¡®ä¿å¯¹é½ï¼šå¦‚æœ words_data å°‘äº raw_wordsï¼Œè¡¥å…¨ Missing å­—æ ·
        # è¿™é˜²æ­¢äº†â€œæœ€åä¸€å¥æ˜¾ç¤ºæ¼è¯»â€
        while len(words_data) < len(raw_words):
             words_data.append([]) # æ’å…¥ç©ºåˆ—è¡¨ä½œä¸ºå ä½
             
        for i, word_segs in enumerate(words_data):
            if i >= len(raw_words): break
            
            word_text = raw_words[i]
            
            if not word_segs:
                 # å…œåº•ï¼šçœŸçš„å®Œå…¨æ²¡æœ‰å¯¹é½åˆ°ä»»ä½• Token
                 score = 0.0
                 tag = WordTag.MISSING
            else:
                avg_gop = np.mean([s["gop"] for s in word_segs])
                
            # Calibrated Score (Gentle Mapping v3 - Ultra Relaxed)
            # ç›®æ ‡ï¼šè®©æ™®é€šä¸­å›½å­¦ç”Ÿçš„æµåˆ©æœ—è¯»ä¹Ÿèƒ½è¾¾åˆ° 80+ï¼Œé‡å£éŸ³ä¹Ÿæœ‰ 60+
            # GOP -12 (æå·®) -> 100 - 12*5 = 40 (Red)
            # GOP -8 (åŠæ ¼) -> 100 - 40 = 60 (Orange)
            # GOP -6 (å°šå¯) -> 100 - 30 = 70 (Orange/Green)
            # GOP -3 (è‰¯å¥½) -> 100 - 15 = 85 (Green)
            score = float(np.clip(100 + avg_gop * 5.0, 0, 100))
            if score < 40 and avg_gop > -20: score = 40 # ä¿æŒç”Ÿå­˜åº•çº¿
            
            tag = WordTag.OK if score >= 75 else (WordTag.WEAK if score >= 45 else WordTag.POOR)
            
            if tag != WordTag.OK:
                weak_words_list.append(word_text)
            
            # --- Phoneme / Detail Processing ---
            phonemes_list = []
            for seg in word_segs:
                # å†…éƒ¨åˆ¤å®šåˆ†ï¼šæ›´ä¸¥å‰ä¸€äº›ï¼Œç¡®ä¿èƒ½åœ¨ mistake_highlights ä¸­ä½“ç°
                # GOP -3 (è‰¯å¥½) -> 100 - 24 = 76 (OK)
                # GOP -6 (å°šå¯) -> 100 - 48 = 52 (WEAK)
                p_score = float(np.clip(100 + seg["gop"] * 8.0, 0, 100))
                p_tag = PhonemeTag.OK if p_score >= 80 else (PhonemeTag.WEAK if p_score >= 60 else PhonemeTag.POOR)
                phonemes_list.append(PhonemeAlignment(
                    phoneme=seg["token"],
                    start=seg["start"],
                    end=seg["end"],
                    score=p_score,
                    tag=p_tag
                ))
            
            w_align = WordAlignment(
                word=word_text,
                start=word_segs[0]["start"],
                end=word_segs[-1]["end"],
                score=score,
                tag=tag,
                phonemes=phonemes_list
            )
            result.words.append(w_align)

        # å…¨å±€è¯„åˆ†
        overall_score = float(np.mean([w.score for w in result.words])) if result.words else 0
        
        # ç”Ÿæˆé›†æˆåé¦ˆ (Rule-based specific feedback)
        integrated_feedback = self._generate_integrated_feedback(weak_words_list, overall_score)
        
        engine_raw = {
            "source": "Wav2Vec2-GOP",
            "overall_score": overall_score,
            "integrated_feedback": integrated_feedback
        }
        
        return result, engine_raw

    def _generate_integrated_feedback(self, weak_words: list[str], overall_score: float) -> dict[str, Any]:
        """
        åŸºäºè§„åˆ™ç”Ÿæˆå…·ä½“çš„å‘éŸ³å»ºè®®ï¼Œä½œä¸º AI è€å¸ˆçš„æ›¿ä»£/å¢å¼ºã€‚
        """
        tips = []
        
        # 1. å¼±è¯»è¯æ±‡å»ºè®®
        if weak_words:
            unique_weak = list(sorted(set(weak_words), key=weak_words.index))[:3]
            tips.append(f"é‡ç‚¹ç»ƒä¹ ä»¥ä¸‹å•è¯çš„å‘éŸ³ï¼š{', '.join(unique_weak)}ã€‚å°è¯•æŠŠæ¯ä¸ªéŸ³èŠ‚å‘é¥±æ»¡ã€‚")
        else:
            tips.append("ä½ çš„å•è¯å‘éŸ³éƒ½å¾ˆæ¸…æ™°ï¼Œéå¸¸æ£’ï¼")
            
        # 2. åªæœ‰æ•´ä½“å¾ˆé«˜åˆ†æ‰å¤¸è‡ªç„¶åº¦
        if overall_score > 85:
            tips.append("æ•´ä½“è¯­æµéå¸¸è‡ªç„¶ï¼Œç»§ç»­ä¿æŒè¿™ç§è‡ªä¿¡çš„è¯­è°ƒï¼")
        elif overall_score < 60:
             tips.append("å°è¯•æ”¾æ…¢è¯­é€Ÿï¼Œå…ˆç¡®ä¿æ¯ä¸ªå•è¯å‘éŸ³å‡†ç¡®ï¼Œå†è¿½æ±‚è¿è´¯æ€§ã€‚")
             
        # 3. è¿™é‡Œçš„ dict ç»“æ„è¦åŒ¹é… runner.py æœŸæœ›çš„ 'integrated' ç»“æ„
        return {
            "overall_comment": "æ•´ä½“è¡¨ç°ä¸é”™ï¼Œ" + ("ä½†åœ¨éƒ¨åˆ†å•è¯çš„å‘éŸ³ç»†èŠ‚ä¸Šå¯ä»¥æ›´ç²¾å‡†ã€‚" if weak_words else "å‘éŸ³æ¸…æ™°æµç•…ï¼"),
            "specific_suggestions": tips,
            "practice_tips": ["æ¯å¤©åšæŒè·Ÿè¯» 10 åˆ†é’Ÿ", "é‡åˆ°éš¾è¯»çš„é•¿éš¾å¥å¯ä»¥æ‹†åˆ†æˆå°èŠ‚ç»ƒä¹ "],
            "fun_challenge": "ğŸŒŸ æŒ‘æˆ˜ï¼šå°è¯•ç”¨è¿™ç§è¯­è°ƒæœ—è¯»ä¸€æ®µä½ æœ€å–œæ¬¢çš„ç”µå½±å°è¯ï¼"
        }
